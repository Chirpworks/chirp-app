# ✅ Use CUDA image with cuDNN runtime
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Prevent interactive prompts & enable unbuffered Python output
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# 🧰 Install system dependencies (including Aeneas & eSpeak-NG prerequisites)
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    ffmpeg \
    libsndfile1 \
    libgl1 \
    libglib2.0-0 \
    python3.10 \
    python3.10-dev \
    python3-pip \
    default-jdk \
    libxml2-dev \
    gfortran \
    cmake \
    espeak \
    libespeak-dev \
    pkg-config \
    build-essential \
  && apt-get clean && rm -rf /var/lib/apt/lists/*

# 🔗 Make python3.10 & pip3 the defaults
RUN ln -sf /usr/bin/python3.10 /usr/bin/python && \
    ln -sf /usr/bin/pip3 /usr/bin/pip

# ⬆️ Upgrade pip and setuptools
RUN pip install --upgrade pip setuptools wheel


# ⚙️ Install GPU-compatible PyTorch (CUDA 11.8)
RUN pip install \
    torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0 \
    --extra-index-url https://download.pytorch.org/whl/cu118

# 🧠 Install WhisperX
RUN pip install git+https://github.com/m-bain/whisperx.git

# 🎙️ Install PyAnnote for diarization
RUN pip install pyannote.audio==3.0.1

# 🎓 Install Aeneas for forced alignment
RUN pip install aeneas

RUN pip install "ctranslate2[cuda11.8]"

# ── Now that all Python packages are in place, set up our cache dirs in a known location ──
# We want both Hugging Face and Transformers to cache under /model_cache
ENV TRANSFORMERS_CACHE=/model_cache
ENV HF_HOME=/model_cache

# 📦 Copy your application code
WORKDIR /app
COPY app/ ./app/

# 📦 Make your app/ importable
ENV PYTHONPATH=/app

# 📜 Install any additional Python dependencies
COPY requirements_speaker_diarization.txt .
RUN pip install -r requirements_speaker_diarization.txt

COPY requirements.txt .
RUN pip install -r requirements.txt

# 11) Remove any half-baked snapshots of the Hindi model (if they exist)
RUN rm -rf /model_cache/hub/models--vasista22--whisper-hindi-large-v2

# 12) Pre-download + pre-convert the vasista22/whisper-hindi-large-v2 model.
#     Wrap everything in one single-quoted bash -c, so Docker doesn’t split on newlines.
RUN bash -c 'python3 - << "EOF"
from faster_whisper import WhisperModel
# Instantiating WhisperModel(...) during build will:
#   1) download the HF weights into /model_cache/hub/models--vasista22--whisper-hindi-large-v2/…
#   2) convert them via CTranslate2, writing model.bin in that snapshot folder.
# We choose device="cuda" + compute_type="float32" so the conversion is GPU-compatible.
_
 = WhisperModel(
    "vasista22/whisper-hindi-large-v2",
    device="cuda",
    compute_type="float32"
)
EOF'

# 🚀 Default entrypoint for serverless runner
ENTRYPOINT ["python", "app/serverless_handler.py"]