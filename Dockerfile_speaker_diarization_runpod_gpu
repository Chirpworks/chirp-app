# âœ… Use CUDA image with cuDNN runtime
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Prevent interactive prompts & enable unbuffered Python output
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# ğŸ§° Install system dependencies (including Aeneas & eSpeak-NG prerequisites)
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    ffmpeg \
    libsndfile1 \
    libgl1 \
    libglib2.0-0 \
    python3.10 \
    python3.10-dev \
    python3-pip \
    default-jdk \
    libxml2-dev \
    gfortran \
    cmake \
    espeak \
    libespeak-dev \
    pkg-config \
    build-essential \
  && apt-get clean && rm -rf /var/lib/apt/lists/*

# ğŸ”— Make python3.10 & pip3 the defaults
RUN ln -sf /usr/bin/python3.10 /usr/bin/python && \
    ln -sf /usr/bin/pip3 /usr/bin/pip

# â¬†ï¸ Upgrade pip and setuptools
RUN pip install --upgrade pip setuptools wheel


# âš™ï¸ Install GPU-compatible PyTorch (CUDA 11.8)
RUN pip install \
    torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0 \
    --extra-index-url https://download.pytorch.org/whl/cu118

# ğŸ§  Install WhisperX
RUN pip install git+https://github.com/m-bain/whisperx.git

# ğŸ™ï¸ Install PyAnnote for diarization
RUN pip install pyannote.audio==3.0.1

# ğŸ“ Install Aeneas for forced alignment
RUN pip install aeneas

RUN pip install "ctranslate2[cuda11.8]"

# â”€â”€ Now that all Python packages are in place, set up our cache dirs in a known location â”€â”€
# We want both Hugging Face and Transformers to cache under /model_cache
ENV TRANSFORMERS_CACHE=/model_cache
ENV HF_HOME=/model_cache

# ğŸ“¦ Copy your application code
WORKDIR /app
COPY app/ ./app/

# ğŸ“¦ Make your app/ importable
ENV PYTHONPATH=/app

# ğŸ“œ Install any additional Python dependencies
COPY requirements_speaker_diarization.txt .
RUN pip install -r requirements_speaker_diarization.txt

COPY requirements.txt .
RUN pip install -r requirements.txt

# 11) Remove any half-baked snapshots of the Hindi model (if they exist)
RUN rm -rf /model_cache/hub/models--vasista22--whisper-hindi-large-v2

# 12) Preâ€download and preâ€convert 'vasista22/whisper-hindi-large-v2' via Fasterâ€Whisper/CTranslate2.
#     By wrapping EVERYTHING in one doubleâ€quoted bash -c, Docker will not split on the newline before 'from'.
RUN bash -c "python3 - << 'EOF'
from faster_whisper import WhisperModel

# Instantiating WhisperModel(...) during build will:
#   1) Download the HF weights under /model_cache/hub/models--vasista22--whisper-hindi-large-v2/â€¦
#   2) Convert them with CTranslate2, writing model.bin into that same snapshot folder.
# We choose device='cuda' and compute_type='float32' so that the GPUâ€compatible CTranslate2 conversion runs.
_ = WhisperModel(
    'vasista22/whisper-hindi-large-v2',
    device='cuda',
    compute_type='float32'
)
EOF"

# ğŸš€ Default entrypoint for serverless runner
ENTRYPOINT ["python", "app/serverless_handler.py"]